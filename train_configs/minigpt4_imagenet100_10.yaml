model:
  arch: mini_gpt4
  model_type: pretrain_vicuna0

  max_txt_len: 160
  end_sym: "###"
  prompt_path: "prompts/alignment.txt"
  prompt_template: '###Human: {} ###Assistant: '
  ckpt: './vicuna-7b/prerained_minigpt4_7b.pth'
  details: "router_methods"
  lora_rank: 4
  lora_nums: 10
  task_num: 10
  shrink: False

dataset: tiny
dataset_root: "./datasets/imagenet/images"
initial_increment: 10
increment: 10
class_num: 100
scenario: class
class_order: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 
12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,
 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 
 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 
 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 
 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 
 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 
 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
# class_order: [173, 157, 89, 198, 44, 195, 183, 127, 129, 166, 95, 167, 187, 76, 108, 54, 181, 68, 170, 130, 6, 197, 47, 3, 104, 172, 96, 21, 112, 88, 168, 150, 49, 126, 109, 70, 40, 171, 37, 39, 1, 43, 160, 61, 82, 103, 69, 111, 107, 33, 31, 80, 78, 119, 152, 53, 18, 186, 131, 178, 10, 140, 143, 66, 45, 122, 62, 121, 58, 115, 139, 79, 151, 30, 134, 177, 141, 34, 164, 196, 101, 63, 11, 14, 87, 71, 42, 184, 25, 7, 4, 145, 98, 123, 144, 188, 84, 35, 190, 48, 194, 159, 179, 5, 20, 93, 83, 81, 94, 99, 65, 185, 52, 26, 136, 174, 137, 38, 155, 59, 165, 191, 148, 12, 158, 97, 27, 180, 135, 67, 8, 75, 110, 132, 128, 64, 192, 105, 153, 56, 189, 102, 142, 74, 182, 13, 176, 86, 175, 85, 163, 73, 57, 114, 23, 50, 117, 146, 15, 77, 16, 120, 36, 92, 193, 51, 60, 149, 156, 118, 32, 199, 19, 113, 22, 147, 0, 116, 154, 125, 133, 28, 55, 124, 2, 9, 72, 24, 100, 162, 17, 161, 169, 91, 106, 46, 41, 90, 29, 138]
n_clusters: 1

datasets:
  cc_sbu_align:
    vis_processor:
      train:
        name: "blip2_image_train"
        image_size: 224
    text_processor:
      train:
        name: "blip_caption"

run:
  task: image_text_pretrain
  # optimizer
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 3e-4
  min_lr: 1e-6
  warmup_lr: 1e-7

  weight_decay: 0.05
  max_epoch: 2
  iters_per_epoch: 200
  batch_size_train: 2
  batch_size_eval: 12
  num_workers: 4
  warmup_steps: 200

  seed: 42
  output_dir: "output/"

  amp: True
  resume_ckpt_path: null

  evaluate: False 
  train_splits: ["train"]

  device: "cuda"
  world_size: 1
  dist_url: "env://"
  distributed: True

task_num: 10